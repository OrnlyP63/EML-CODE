{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1104f56-a601-40d7-a085-3fa1d2be8dcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Semi-Supervised Classification via Hypergraph Convolutional Extreme Learning Machine\n",
    "\n",
    "\n",
    "### Zhewei Liu , Zijia Zhang , Yaoming Cai , Yilin Miao and Zhikun Chen ;2021\n",
    "\n",
    "School of Computer Science, China University of Geosciences, Wuhan, China;\n",
    "\n",
    "#### Presented by Phiphat Chomchit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb02de13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypergraph Convolutional Extreme Learning Machine (HGCELM)\n",
    "\n",
    "- proposed by Zhewei Liu et al, 2021\n",
    "- Semi Supervised Learning + Graph Covolutional Network + Extreme Learning Machine\n",
    "- Graph Convolution Extreme Learning Machine\n",
    "- Hyper Graph vs. Pairwise Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f288b9ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Extreme Learning Machine\n",
    "\n",
    "- Huang et al, 2004\n",
    "- Single Hidden Layer Feedforward Neural Networks\n",
    "- Supervised --> Classifies probelm and Regression problem\n",
    "- Does not need update during training\n",
    "- SVM: kernel is a mapping the data; ELM: kernel is a random mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fcfeba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='ELM.png'>\n",
    "\n",
    "Kasun, 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b4bfc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Learning Era.\n",
    "\n",
    "- **Classic ELM** : Poor robustness because random mapping.\n",
    "- **multi-objective evolutional ELM** : heuristic search --> often time-consuming.\n",
    "- **Kernel ELM** : SVM, hidden random mapping in Hilbert space. (Analogizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b78b3c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='svm.png' width=1000>\n",
    "Phimphaka, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae203b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Semi-Supervised Learning Era.\n",
    "- **SS-ELM** : Graph Lapacian regularization., **Pairwise relationship.**\n",
    "- **GCN** : massage passing, gradient descent. --> Over smoothing probelm.\n",
    "- **GCELM** : SS-ELM + GCN --> (randomization-based GCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52821a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objective of this paper\n",
    "- Propose a simple but effective **hypergraph convolutional ELM**, i.e., HGCELM, for semi-supervised classification.\n",
    "- HGCELM (Hyper Graph) vs. GCELM (Pairwise relationship)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a7992",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classical ELM\n",
    "\n",
    "- **Activate function matrix** (Random mapping)\n",
    "\n",
    "Let  $h(x) = g(x,w,c)$ be an activation function.\n",
    "\n",
    "    - Sigmoid Function\n",
    "$$g(x, w, c) = \\frac{1}{1 + e^{-(wx + c)}}$$\n",
    "\n",
    "where $c, w \\sim \\mathcal{N}(0,\\,1)\\,.$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e799c11c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let $H$ be an activate function matrix and   **input data** $X = [x_1, x_2, \\cdots, x_N]^T$, $x\\in \\mathbb{R}^M$,\n",
    "where $M$ is a number of **feature data**\n",
    "\n",
    "$$H = \\begin{bmatrix}\n",
    "h(x_1) & \\\\\n",
    "\\vdots & \\\\\n",
    "h(x_N) & \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "h_1(x_1) & \\cdots & h_L(x_1) \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "h_1(x_N) & \\cdots & h_L(x_N)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$H =  \n",
    "\\begin{bmatrix}\n",
    "g(w_1\\cdot x_1 + c_1) & \\cdots & g(w_L\\cdot x_1 + c_L) \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "g(w_1\\cdot x_N + c_1) & \\cdots & g(w_L\\cdot x_N + c_L)\n",
    "\\end{bmatrix}_{N\\times L}$$\n",
    "\n",
    "\n",
    "where  $N$ is a number of **input data**, and $L$ be a number of **hidden node** , "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2b2fd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='ELM.png'>\n",
    "\n",
    "Kasun, 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc678e3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Beta matrix**\n",
    "\n",
    "\n",
    "\n",
    "Let \n",
    "\n",
    "$$\\beta \\in \\mathbb{R}^{L\\times D},\\quad \\beta = [\\beta_1, \\beta_2, \\cdots, \\beta_L]^T$$\n",
    "\n",
    "be **weight** between hidden node and output data (beta matirx).\n",
    "\n",
    " where $D$ be a number of **output data** . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031e8be0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$f(x) = h(x)\\beta$\n",
    "\n",
    "* Objective\n",
    "\n",
    "$$\\underset{\\beta}{\\mathrm{min}} \\|H\\beta - Y\\|^2$$ \n",
    "\n",
    "So,\n",
    "\n",
    "$$\\beta = H^\\dagger Y$$\n",
    "\n",
    "where $H^\\dagger$ is psudo inverse matrix (**Mooreâ€“Penrose inverse**) of H.\n",
    "$$H^\\dagger = (H^TH)^{-1}H^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1917f2ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM\n",
    "\n",
    "<img src='svm2.png' width=900>\n",
    "Phimphaka, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c292d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='svm3.png' width=1000>\n",
    "Phimphaka, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7975d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='img15.gif' width =700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1e871a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='16.gif' width =700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81015a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kernel ELM\n",
    "\n",
    "Need to minimize $\\|H\\beta - Y\\|^2$ and $\\|\\beta\\|^2$\n",
    "\n",
    "$$\\underset{\\beta,\\xi}{min}\\frac{1}{2}\\|\\beta\\|^2 + c\\frac{1}{2}\\sum_{i=1}^N\\|\\xi_i\\|^2$$\n",
    "\n",
    "s.t. $\\quad h(x)\\beta = y_i^T - \\xi_i^T,\\quad i = 1, ..., N$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb1094",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Lagrange Multiplier Method**\n",
    "\n",
    "$$L = \\frac{1}{2}\\|\\beta\\|^2 + c\\frac{1}{2}\\sum_{i=1}^N\\|\\xi_i\\|^2 - \\sum_{i=1}^N\\sum_{j=1}^M\\alpha_{i,j}(h(x_i)\\beta_j - y_{i,j} + \\xi_{i,j})$$\n",
    "\n",
    "where $\\xi_i = [\\xi_{i,1}, ..., \\xi_{i,M}]^T$ is the training error vetor of the $M$ output nodes\n",
    "\n",
    "and  $\\quad\\alpha_i = [\\alpha_{i,1}, ..., \\alpha_{i,M}]^T$ is the Lagrange multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ff954",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Critical points**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial{L}}{\\partial{\\beta_i}} &&= 0 \\rightarrow &&\\beta_j = \\sum_{i=1}^N \\alpha_{i,j}h(x_i)^T \\rightarrow \\beta = H^T\\alpha \\quad (1)\\\\\n",
    "\\frac{\\partial{L}}{\\partial{\\xi_i}} &&= 0 \\rightarrow &&\\alpha_i = c\\xi_i,\\quad i=1,...,N\\quad (2)\\\\\n",
    "\\frac{\\partial{L}}{\\partial{\\alpha_i}} &&= 0 \\rightarrow &&h(x_i)\\beta - y^T_i + \\xi^T_i = 0,\\quad i=1,...,N\\quad (3)\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3278e58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Solve Equation**\n",
    "\n",
    "From $(2)$ implies $\\xi_i = \\frac{\\alpha_i}{c}$.\n",
    "\n",
    "Consider $(3)$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h(x_i)\\beta - y^T_i + \\xi^T_i &= 0 \\\\\n",
    "h(x_i)\\beta - y^T_i + \\frac{\\alpha_i^T}{c} &= 0\\\\\n",
    "H\\beta - Y + \\frac{\\alpha}{c} &= 0\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa1e52d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "Y &= H\\beta + \\frac{\\alpha}{c}\\\\\n",
    "Y &= (HH^T\\alpha + \\frac{\\alpha}{c})\\quad \\text{by}\\,(1)\\\\\n",
    "Y &= (HH^T + \\frac{I}{c})\\alpha\\\\\n",
    "\\alpha &= (HH^T + \\frac{I}{c})^{-1}Y\\\\\n",
    "\\beta &= H^T(HH^T + \\frac{I}{c})^{-1}Y\\quad \\text{by}\\,(1)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84858b88",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GCN\n",
    "\n",
    "* Adjacent matrix (Graph representation): $A$\n",
    "\n",
    "example:\n",
    "<img src='A.png' width=700> _Zhewei Liu, 2021_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a586f58b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Let $\\tilde{A} = I_N + A$ be the augmented normalized adjacency.\n",
    "\n",
    "$$\\tilde{A} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 1 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0 & 0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0 & 1 & 0 & 0\\\\\n",
    "1 & 0 & 0 & 1 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 1 & 0 & 0 & 1\\\\\n",
    "1 & 0 & 1 & 1 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0 & 0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 1 & 0 & 0 & 1\\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f8904c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Define $\\tilde{D}$ by $\\tilde{D_{ii}} = \\sum_j \\tilde{A}_{ij}$.\n",
    "\n",
    "$$\\tilde{D} = \\begin{bmatrix}\n",
    "3 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 2 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 3 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 2 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 4 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 2 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 2\\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584b926a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* The normalized hypergraph Laplacian matrix.\n",
    "\n",
    "$$L = I - D_v^{-1/2}ZWD_e^{-1}Z^TD_v^{-1/2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb68a311",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Kift et al.** developed a **Graph Convolutional Network (GCN)** by simplifying\n",
    "the spectral convolution with the 1th-order Chebyshev polynomials and setting the largest\n",
    "eigenvalue of the **normalized graph Laplacian**.\n",
    "\n",
    "Formally, GCN defines spectral convolution over a graph as follows.\n",
    "\n",
    "$$H = h(\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}XW)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ff8cc1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HGCELM\n",
    "\n",
    "* Hypergraph\n",
    "\n",
    "Let $ \\mathcal{G} = (\\mathcal{V, E, W})$ be a **hypergraph** composed of \n",
    "\n",
    "a **vertex set** $\\mathcal{v\\in V}$ with the size of $N$, \n",
    "\n",
    "a **hyperedge set** $\\mathcal{e\\in E}$ with the size $|\\mathcal{E}|$ , \n",
    "\n",
    "and a **weight set of hyperedge** $\\mathcal{W}$ where\n",
    "the weight of hyperedge  $\\mathcal{e}$  e is indicated as $\\mathcal{w(e)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c06b4e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*  Incidence matrix: $Z$\n",
    "\n",
    "<img src='B.png' width = 500> _Zhewei Liu, 2021_\n",
    "\n",
    "Mathematically, the incidence matrix is defined by\n",
    "\n",
    "$$\\mathcal{z( v, e)} = \\begin{cases}\n",
    "1 & \\mathcal{v}\\in \\mathcal{e}\\\\\n",
    "0 & \\mathcal{v}\\not\\in \\mathcal{e}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca6925e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The degree of a vertex\n",
    "$$\\mathcal{d(v) = \\sum_{e\\in E}w(e)z(v, e)}$$\n",
    "\n",
    "*  degree of a hyperedge\n",
    "$$\\mathcal{\\delta(e) = \\sum_{v\\in V}z(v, e)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82570ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The diagonal matrix of the vertex degrees, $\\mathbb{R}^{N\\times N}$\n",
    "\n",
    "$$D_{\\mathcal{v}} = \n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 2 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 3 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 2 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 2\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b07453",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The diagonal matrix of the hyperedge degrees, $\\mathbb{R}^{\\mathcal{|E|\\times|E|}}$\n",
    "\n",
    "$$D_{\\mathcal{e}} = \n",
    "\\begin{bmatrix}\n",
    "3 & 0 & 0 & 0\\\\\n",
    "0 & 4 & 0 & 0\\\\\n",
    "0 & 0 & 4 & 0\\\\\n",
    "0 & 0 & 0 & 4\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56cd19e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*  **Hypergraph Construction**\n",
    "\n",
    "    - data point $x_i$ is treated as a vertex $\\mathcal{v}$\n",
    "    - consider that $x_i$ is the center vertex and its $k$ nearest neighbors are associated with the hyperedge $\\mathcal{e}$\n",
    "    \n",
    "$$\\mathcal{z( v_i, e_j)} = \\begin{cases}\n",
    "1 & x_j\\in \\mathcal{N}_k(x_i)\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0974fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Random Hypergraph Convolution**\n",
    "\n",
    "    - Propose a novel ELM mapping, called random hypergraph convolution (RGC). It inspired from GCELM.\n",
    "\n",
    "$$H = h(SXW)$$\n",
    "\n",
    "where $S = D_{\\mathcal{v}}^{-1/2} Z D_{\\mathcal{e}}^{-1} Z^{-1} D_{\\mathcal{v}^{-1/2}}\\quad$ and $\\quad W_{ij}\\sim \\mathcal{N(0, 1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e386ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Hypergraph Convolutional Regression**\n",
    "Hypergraph Convolutional Regression $H$. Formally, the layer can be written as:\n",
    "\n",
    "$$Y = SH\\beta$$\n",
    "\n",
    "To solve $\\beta$, we need to solve minimization problem:\n",
    "\n",
    "$$\\underset{\\beta^{*}}{min}\\|\\beta\\|_F$$\n",
    "\n",
    "$\\tilde{Y} = [Y_{\\mathcal{T}}; Y_{\\mathcal{U}}]$ is an augmented training target matrix. Set $Y_{\\mathcal{U}}$ to be a $0$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e387881",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " Let $M$ be a diagonal mask matrix with its first $N_{\\mathcal{T}}$ diagonal elements $M_{ii} = 1$ and the rest equal to $0$. We rewrite above equation as\n",
    " \n",
    "$$\\underset{\\beta^{*}}{min}\\frac{1}{2}\\|M(\\tilde{Y} - SH\\beta)\\|^2_F + \\frac{\\lambda}{2}\\|\\beta\\|^2_F$$\n",
    "\n",
    "* A optimal solution.\n",
    "\n",
    "$$\\beta^* = (H^TS^TMSH + \\lambda I)^{-1}H^TS^TM\\tilde{Y}.$$\n",
    "\n",
    "* The labels of the unlabeled data points\n",
    "\n",
    "$$\\bar{Y}_{\\mathcal{U}} = [SH]_{\\mathcal{U}}\\beta^*$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70badecf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Algorithm: HGCELM**\n",
    "\n",
    "    **Input**: Dataset $X$, training label $Y_{\\mathcal{T}}$, hyper-parameters $\\lambda$ and $L.$\n",
    "    \n",
    "    1.) Construct hypergraph $\\mathcal{G = (V, E, W)}$;\n",
    "    \n",
    "    2.) Generate $W$ using the  standard normal distribution;\n",
    "    \n",
    "    3.)  Calculate random hypergraph embedding: $H = h(SXW)$;\n",
    "    \n",
    "    4.) Solve hypergraph conv regression: $\\beta^* = (H^TS^TMSH + \\lambda I)^{-1}H^TS^TM\\tilde{Y}. $;\n",
    "    \n",
    "    5.) Predict test levels by $\\bar{Y}_{\\mathcal{U}} = [SH]_{\\mathcal{U}}\\beta^*$;\n",
    "    \n",
    "    **Output** $\\bar{Y}_{\\mathcal{U}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e88b722",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data sets\n",
    "\n",
    "<img src='2.png' width=1000> _Zhewei Liu, 2021_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd89c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model and Hyperparameters\n",
    "<img src='3.png' width=800> _Zhewei Liu, 2021_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7504816",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Results\n",
    "\n",
    "<img src='4.png' width=900> _Zhewei Liu, 2021_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec538cfc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visulization Results 1\n",
    "\n",
    "<img src='F3.png' width=700> _Zhewei Liu, 2021_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cadc10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visulization Results 2\n",
    "\n",
    "<img src='F4.png' width=640> _Zhewei Liu, 2021_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0652b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$Q/A$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python (AIML)",
   "language": "python",
   "name": "aiml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
