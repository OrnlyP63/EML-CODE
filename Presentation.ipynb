{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1104f56-a601-40d7-a085-3fa1d2be8dcb",
   "metadata": {},
   "source": [
    "# Semi-Supervised Classification via Hypergraph Convolutional Extreme Learning Machine\n",
    "\n",
    "\n",
    "### Zhewei Liu , Zijia Zhang , Yaoming Cai , Yilin Miao and Zhikun Chen \n",
    "\n",
    "#### Presented by Phiphat Chomchit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea59d5eb",
   "metadata": {},
   "source": [
    "# Hypergraph Convolutional Extreme Learning Machine (HGCELM)\n",
    "\n",
    "- propose by Zhewei Liu et al, 2021\n",
    "- Semi Supervised Learning + Graph Covolutional Network + Extreme Learning Machine\n",
    "- Graph Convolution Extreme Learning Machine\n",
    "- Hyper Graph vs. Pairwise Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654645d3",
   "metadata": {},
   "source": [
    "#  Extreme Learning Machine\n",
    "\n",
    "- Huang et al, 2004\n",
    "- Single Hidden Layer Feedforward Neural Networks\n",
    "- supervised -> Classifies probelm and Regression problem\n",
    "- SVM (เรียนรู้ได้รวดเร็วมาก) random mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693e88bd",
   "metadata": {},
   "source": [
    "# Supervised Learning Era.\n",
    "\n",
    "- **Classic ELM** : Poor robustness because random mapping.\n",
    "- **multi-objective evolutional ELM** : heuristic search -> often time-consuming.\n",
    "- **Kernel ELM** : SVM, hidden random mapping in Hilbert space. (Analogizer)\n",
    "- **Other** : ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3be9f",
   "metadata": {},
   "source": [
    "# Semi-Supervised Learning Era.\n",
    "- **SS-ELM** : Graph Lapacian regularization., Pairwise relationship node.\n",
    "- **GCN** : massage passing, gradient decent. -> over smoothing probelm.\n",
    "- **GCELM** : SS-ELM + GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1d5ad",
   "metadata": {},
   "source": [
    "# Objective of this paper\n",
    "- Hyper Graph.\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631fe9b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classic ELM\n",
    "\n",
    "- **Activate function matrix** (Random mapping)\n",
    "\n",
    "Let  $h(x) = g(x,w,c)$ be an activation function.\n",
    "\n",
    "    - Sigmoid Function\n",
    "$$g(x, w, c) = \\frac{1}{1 + e^{-(wx + c)}}$$\n",
    "\n",
    "where $c, w \\sim \\mathcal{N}(0,\\,1)\\,.$\n",
    "\n",
    "Let $H$ be an activat function matrix.\n",
    "\n",
    "$$H = \\begin{bmatrix}\n",
    "h(x_1) & \\\\\n",
    "\\vdots & \\\\\n",
    "h(x_N) & \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "h_1(x_1) & \\cdots & h_L(x_1) \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "h_1(x_N) & \\cdots & h_L(x_N)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$H =  \n",
    "\\begin{bmatrix}\n",
    "g(w_1\\cdot x_1 + c_1) & \\cdots & g(w_L\\cdot x_1 + c_L) \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "g(w_1\\cdot x_N + c_1) & \\cdots & g(w_L\\cdot x_N + c_L)\n",
    "\\end{bmatrix}_{N\\times L}$$\n",
    "\n",
    "\n",
    "where  $N$ is a number of **input data**,\n",
    "\n",
    "and $L$ be a number of **hidden node** , "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f4b435",
   "metadata": {},
   "source": [
    "* **Beta matrix**\n",
    "\n",
    "Let **input data** $X = [x_1, x_2, \\cdots, x_N]^T$, $x\\in \\mathbb{R}^M$,\n",
    "\n",
    "Let \n",
    "\n",
    "$$\\beta \\in \\mathbb{R}^{L\\times D},\\quad \\beta = [\\beta_1, \\beta_2, \\cdots, \\beta_L]^T$$\n",
    "\n",
    "be **weight** between hidden node and output data (beta matirx).\n",
    "\n",
    "where $M$ is a number of **feature data**  and $D$ be a number of **output data**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6886ae",
   "metadata": {},
   "source": [
    "### Objective\n",
    "$$\\underset{\\beta}{\\mathrm{min}} \\|H\\beta - Y\\|^2$$ \n",
    "\n",
    "So,\n",
    "\n",
    "$$\\beta = H^\\dagger Y$$\n",
    "\n",
    "where $H^\\dagger$ is psudo inverse matrix (**Moore–Penrose inverse**) of H.\n",
    "$$H^\\dagger = (H^TH)^{-1}H^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a560e167",
   "metadata": {},
   "source": [
    "# Kernel ELM\n",
    "\n",
    "$$\\underset{\\beta,\\xi}{min}\\frac{1}{2}\\|\\beta\\|^2 + c\\frac{1}{2}\\sum_{i=1}^N\\|\\xi_i\\|^2$$\n",
    "\n",
    "s.t.\n",
    "\n",
    "$h(x)\\beta = y_i^T - \\xi_i^T,\\quad i = 1, ..., N$\n",
    "\n",
    "* **Lagrange Multiplier Method**\n",
    "\n",
    "$$L = \\frac{1}{2}\\|\\beta\\|^2 + c\\frac{1}{2}\\sum_{i=1}^N\\|\\xi_i\\|^2 - \\sum_{i=1}^N\\sum_{j=1}^M\\alpha_{i,j}(h(x_i)\\beta_j - y_{i,j} + \\xi_{i,j})$$\n",
    "\n",
    "where $\\xi_i = [\\xi_{i,1}, ..., \\xi_{i,M}]^T$ is the training error vetor of the $M$ output nodes\n",
    "\n",
    "and  $\\quad\\alpha_i = [\\alpha_{i,1}, ..., \\alpha_{i,M}]^T$ is the Lagrange multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b4e98e",
   "metadata": {},
   "source": [
    "* **Critical points**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial{L}}{\\partial{\\beta_i}} &&= 0 \\rightarrow &&\\beta_j = \\sum_{i=1}^N \\alpha_{i,j}h(x_i)^T \\rightarrow \\beta = H^T\\alpha \\quad (1)\\\\\n",
    "\\frac{\\partial{L}}{\\partial{\\xi_i}} &&= 0 \\rightarrow &&\\alpha_i = c\\xi_i,\\quad i=1,...,N\\quad (2)\\\\\n",
    "\\frac{\\partial{L}}{\\partial{\\alpha_i}} &&= 0 \\rightarrow &&h(x_i)\\beta - y^T_i + \\xi^T_i = 0,\\quad i=1,...,N\\quad (3)\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e47d3c1",
   "metadata": {},
   "source": [
    "* **Solve Equation**\n",
    "\n",
    "From $(2)$ implies $\\xi_i = \\frac{\\alpha_i}{c}$.\n",
    "\n",
    "Consider $(3)$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h(x_i)\\beta - y^T_i + \\xi^T_i &= 0 \\\\\n",
    "h(x_i)\\beta - y^T_i + \\frac{\\alpha_i^T}{c} &= 0\\\\\n",
    "H\\beta - Y + \\frac{\\alpha}{c} &= 0\\\\\n",
    "Y &= H\\beta + \\frac{\\alpha}{c}\\\\\n",
    "Y &= (HH^T\\alpha + \\frac{\\alpha}{c})\\quad \\text{by}\\,(1)\\\\\n",
    "Y &= (HH^T + \\frac{I}{c})\\alpha\\\\\n",
    "\\alpha &= (HH^T + \\frac{I}{c})^{-1}Y\\\\\n",
    "\\beta &= H^T(HH^T + \\frac{I}{c})^{-1}Y\\quad \\text{by}\\,(1)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17a65b",
   "metadata": {},
   "source": [
    "# GCN\n",
    "\n",
    "* Adjacent matrix (Graph representation): $A$\n",
    "\n",
    "example:\n",
    "<img src='A.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82eac44",
   "metadata": {},
   "source": [
    "* Let $\\tilde{A} = I_N + A$ be the augmented normalized adjacency.\n",
    "\n",
    "$$\\tilde{A} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 1 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0 & 0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0 & 1 & 0 & 0\\\\\n",
    "1 & 0 & 0 & 1 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 1 & 0 & 0 & 1\\\\\n",
    "1 & 0 & 1 & 1 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0 & 0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 1 & 0 & 0 & 1\\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fbccaf",
   "metadata": {},
   "source": [
    "* Define $\\tilde{D}$ by $\\tilde{D_{ii}} = \\sum_j \\tilde{A}_{ij}$.\n",
    "\n",
    "$$\\tilde{D} = \\begin{bmatrix}\n",
    "3 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 2 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 3 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 2 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 4 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 2 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 2\\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af6ab1",
   "metadata": {},
   "source": [
    "* The matrix of latent representation\n",
    "\n",
    "$$H = h(\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}XW)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de03d858",
   "metadata": {},
   "source": [
    "# HGCELM\n",
    "\n",
    "* Hypergraph\n",
    "\n",
    "Let $ \\mathcal{G} = (\\mathcal{V, E, W})$ be a **hypergraph** composed of \n",
    "\n",
    "a **vertex set** $\\mathcal{v\\in V}$ with the size of $N$, \n",
    "\n",
    "a **hyperedge set** $\\mathcal{e\\in E}$ with the size $|\\mathcal{E}|$ , \n",
    "\n",
    "and a **weight set of hyperedge** $\\mathcal{W}$ where\n",
    "the weight of hyperedge  $\\mathcal{e}$  e is indicated as $\\mathcal{w(e)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2212e8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*  Incidence matrix: $Z$\n",
    "\n",
    "<img src='B.png' width = 500>\n",
    "\n",
    "Mathematically, the incidence matrix is defined by\n",
    "\n",
    "$$\\mathcal{z( v, e)} = \\begin{cases}\n",
    "1 & \\mathcal{v}\\in \\mathcal{e}\\\\\n",
    "0 & \\mathcal{v}\\not\\in \\mathcal{e}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3956a45",
   "metadata": {},
   "source": [
    "* The normalized hypergraph Laplacian matrix.\n",
    "\n",
    "$$L = I - D_v^{-1/2}ZWD_e^{-1}Z^TD_v^{-1/2}$$\n",
    "\n",
    "* The degree of a vertex\n",
    "$$\\mathcal{d(v) = \\sum_{e\\in E}w(e)z(v, e)}$$\n",
    "\n",
    "*  degree of a hyperedge\n",
    "$$\\mathcal{\\delta(e) = \\sum_{v\\in V}z(v, e)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2abb31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python (AIML)",
   "language": "python",
   "name": "aiml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
